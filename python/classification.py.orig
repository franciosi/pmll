import numpy as np

class Irls(object):
    def __init__(self, weights=None):
        # 'weights' are used for 1) init weights 2) create model
        self.weights = weights
        self.__history = {'weights': self.weights}

    def __logit(self, z): return 1 / (1 + np.exp(-z))

    def classify(self, objects):
        #  return logit function of linear regression
        return self.__logit(self.get_linear_regression(objects))

    def get_linear_regression(self, objects):
        # convert input to matrix, append constant column
        # return linear regression
        if self.weights is None:
            raise ValueError('weights are not assignment. Current value: %s' % self.weights)
        else:
            return np.asmatrix(np.hstack([np.asmatrix(objects),
                                          np.ones([objects.shape[0], 1])
                                          ])) * self.weights

    def train(self, objects, labels, object_weights=None, max_iterations=100, accuracy=1e-5,
              regularization_parameter=1e-5):
        # Convert input to np.matrix. Append 1 to each object.
        objects = np.hstack([np.asmatrix(objects), np.ones([objects.shape[0], 1])])
        labels = np.asmatrix(labels)
        if object_weights is None:
            object_weights = np.array([[1]] * objects.shape[0])

        if self.weights is None:
            self.weights = np.random.randn(objects.shape[1], 1) / objects.shape[1]
            self.__history['weights'] = self.weights

        for iteration in range(max_iterations):
            probability = self.classify(objects[:,:-1]) # without 1
            B = np.diagflat(np.multiply(probability - np.power(probability, 2), object_weights))

            # Dont use -=, it changes history[weights]
            self.weights = self.weights - (objects.T * B * objects + \
                 regularization_parameter * np.eye(objects.shape[1])) ** (-1) *\
                 objects.T * (probability - labels)

            self.__history['weights'] = np.hstack([self.__history['weights'],
                                                   self.weights])

            if sum(abs(self.__history['weights'][:, -1] - \
                           self.__history['weights'][:, -2])) < accuracy:
                break

    def plot_convergence(self):
        from matplotlib import pyplot as plt
        for weight_index, weights in enumerate(self.__history['weights']):
            plt.plot(self.__history['weights'].T)
            plt.show()
            # print '{\nname: \"weight#%s\",' % weight_index
            # print 'data: ', list(np.asarray(weights).flat), '\n},'

    def __str__(self):
        return '%s, weights:\n%s' % (self.__class__, str(self.weights))


class EmIrls(object):
    def __init__(self, number_models, weights=None):
        self.number_models = number_models
        self.weights = weights

    def __str__(self):
        return "%s, weights:\n%s" % (self.__class__, str(self.weights))

    def choose_model(self, objects):
        algo = Irls(self.weights)
        return abs(algo.get_linear_regression(objects)).argmin(1)

    def __get_classifier(self):
        return self
    classifier = property(__get_classifier)

    def train(self, objects, labels, number_iteration=20, verbosity=False):
        objects, labels = np.asmatrix(objects), np.asmatrix(labels)
        self.weights = np.empty([objects.shape[1] + 1, self.number_models])
        self.__object_model = np.random.randint(0, self.number_models, [objects.shape[0], 1])

        for iteration in range(number_iteration):
            if verbosity:
                print "%s Iteration:\t%s" % (self.__class__, iteration)
            # print iteration
            # M-step
            # Estimate the most probable model parameters using objects, assigned to model
            for model_index in range(self.number_models):
                indeces = np.asarray((self.__object_model == model_index).nonzero()[0].flat)
                algo = Irls()
                algo.train(objects[indeces, :], labels[indeces])
                self.weights[:, model_index] = np.asarray(algo.weights.flat)

            # E-step
            # Choose models for objects with gives minimal value to |x * w|
            self.__object_model = self.choose_model(objects)

    def classify(self, objects):
        if self.weights is None:
            raise ValueError("Current weights are None")

        objects = np.asmatrix(objects)
        labels = np.empty([objects.shape[0], 1])

        # Choose models for objects with gives minimal value to |x * w|
        object_model = self.choose_model(objects)

        # Classify objects
        for model_index in range(self.number_models):
            algo = Irls(self.weights[:, model_index][:,np.newaxis])
            indeces = np.asarray((object_model == model_index).nonzero()[0].flat)
            labels[indeces] = algo.classify(objects[indeces, :])

        return labels


class ModelMixturer(object):
    def __init__(self, number_models):
        self.number_models = number_models
        self.model_prior_probability = self.__normalize_rows(np.random.random([1, number_models]))
        self.model_object_probability = None
        self.algorithms = [Irls() for index in range(self.number_models)]

    def __str__(self):
        return "%s\nprior probabilities: %s\nweights:\n%s" % (self.__class__,
                                                          self.model_prior_probability,
                                                          np.hstack(a.weights for a in self.algorithms))

    @staticmethod
    def __normalize_rows(matrix):
        matrix = np.asmatrix(matrix)
        return np.diag(list((1.0 / matrix.sum(1)).flat)) * matrix

    def classify(self, objects):
        return self.get_model_object_probability(objects, normalize=False).sum(1)

    def get_model_object_probability(self, objects, normalize=True):
        model_prior_probability = list(self.model_prior_probability.flat)
        model_object_probability = np.hstack((model_prior_probability[index] * self.algorithms[index].classify(objects)
                                              for index in range(self.number_models)))
        # print model_object_probability

        if normalize:
            return self.__normalize_rows(model_object_probability)
        else:
            return model_object_probability

    def train(self, objects, labels, number_iteration=20, verbosity=False):
        if self.model_object_probability is None:
            self.model_object_probability = np.tile(self.model_prior_probability, (objects.shape[0], 1))

        for iteration in range(number_iteration):
            if verbosity:
                print "%s Iteration:\t%s" % (self.__class__, iteration)

            # M-step (train algorithms)
            self.model_prior_probability = self.model_object_probability.mean(0)
            # print self.model_object_probability
            for index in range(self.number_models):
                object_weights = self.model_object_probability[:, index]
                self.algorithms[index].train(objects, labels, object_weights=object_weights)

            # E-Step
            self.model_object_probability = self.get_model_object_probability(objects)

if __name__ == "__main__":
    import doctest
    doctest.testfile("%s.test" % __file__.split(".", 1)[0])
    # doctest.testfile("%s.test" % __file__.split(".", 1)[0], verbose=True)
